"""\nNested Learning State Space Model (NL-SSM)\n\nImplements multi-time-scale updates inspired by Google's Nested Learning paradigm.\nEach level operates at different frequencies, enabling hierarchical memory consolidation\nsimilar to human brain's synaptic and systems consolidation.\n\nReferences:\n    - Nested Learning: The Illusion of Deep Learning Architectures (NeurIPS 2025)\n    - https://abehrouz.github.io/files/NL.pdf\n"""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Dict, Optional, Tuple, List\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass ContinuumMemoryBlock(nn.Module):\n    \"\"\"\n    Single memory block in Continuum Memory System with proper gradient consolidation.\n    \n    Implements true Nested Learning by maintaining separate fast and slow parameters.\n    Fast weights adapt every step, while slow weights consolidate accumulated gradients\n    at lower frequencies, mimicking synaptic and systems consolidation in neuroscience.\n    \n    Args:\n        input_dim: Input feature dimension\n        hidden_dim: Hidden layer dimension for transformation\n        update_frequency: Number of steps between slow weight updates (1 = every step)\n        dropout: Dropout probability\n        consolidation_rate: Learning rate for slow weight consolidation (default: 0.01)\n    \"\"\"\n    \n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        update_frequency: int = 1,\n        dropout: float = 0.1,\n        consolidation_rate: float = 0.01,\n    ):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.update_frequency = update_frequency\n        self.consolidation_rate = consolidation_rate\n        \n        # Separate fast and slow parameters (key innovation)\n        self.fast_weights = nn.Parameter(torch.randn(hidden_dim, input_dim) * 0.02)\n        self.fast_bias = nn.Parameter(torch.zeros(hidden_dim))\n        \n        self.slow_weights = nn.Parameter(torch.randn(hidden_dim, input_dim) * 0.02)\n        self.slow_bias = nn.Parameter(torch.zeros(hidden_dim))\n        \n        # Transformation layers\n        self.transform = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n        )\n        \n        # Output projection back to input_dim\n        self.output_proj = nn.Linear(hidden_dim, input_dim)\n        \n        # Layer normalization for stability\n        self.layer_norm = nn.LayerNorm(input_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Gradient accumulator for slow weights (NOW ACTUALLY USED)\n        self.register_buffer('accumulated_grad_w', torch.zeros_like(self.slow_weights))\n        self.register_buffer('accumulated_grad_b', torch.zeros_like(self.slow_bias))\n        self.register_buffer('step_counter', torch.zeros(1, dtype=torch.long))\n        \n        # Context compression state\n        self.register_buffer('compressed_context', torch.zeros(input_dim))\n        \n        # Learnable interpolation weight\n        self.alpha = nn.Parameter(torch.tensor(0.5))\n        \n    def forward(\n        self,\n        x: torch.Tensor,\n        force_update: bool = False,\n    ) -> Tuple[torch.Tensor, bool]:\n        \"\"\"\n        Forward pass with conditional consolidation.\n        \n        Args:\n            x: Input tensor [batch, input_dim]\n            force_update: Force slow weight update regardless of frequency\n            \n        Returns:\n            Tuple of (output tensor, was_updated flag)\n        \"\"\"\n        batch_size = x.size(0)\n        should_update = force_update or (self.step_counter % self.update_frequency == 0)\n        \n        # Fast path: always active, adapts every step\n        fast_hidden = F.linear(x, self.fast_weights, self.fast_bias)\n        fast_transformed = self.transform(fast_hidden)\n        fast_output = self.output_proj(fast_transformed)\n        \n        # Slow path: uses cached context when not updating\n        if should_update:\n            # Consolidate accumulated gradients into slow weights\n            if self.training and self.step_counter > 0:\n                self._consolidate_gradients()\n            \n            # Update compressed context with current batch statistics\n            with torch.no_grad():\n                self.compressed_context = x.mean(dim=0).detach()\n            \n            # Compute slow output with updated weights\n            slow_hidden = F.linear(x, self.slow_weights, self.slow_bias)\n            slow_transformed = self.transform(slow_hidden)\n            slow_output = self.output_proj(slow_transformed)\n        else:\n            # Use cached context to reduce computation\n            cached_input = 0.9 * x + 0.1 * self.compressed_context.unsqueeze(0)\n            slow_hidden = F.linear(cached_input, self.slow_weights.detach(), self.slow_bias.detach())\n            slow_transformed = self.transform(slow_hidden)\n            slow_output = self.output_proj(slow_transformed)\n        \n        # Adaptive interpolation between fast and slow paths\n        alpha = torch.sigmoid(self.alpha)\n        output = (1 - alpha) * fast_output + alpha * slow_output\n        \n        # Apply normalization and dropout\n        output = self.dropout(self.layer_norm(output))\n        \n        self.step_counter += 1\n        return output, should_update\n    \n    def _consolidate_gradients(self):\n        \"\"\"\n        Consolidate accumulated gradients into slow weights.\n        This implements the core Nested Learning parameter consolidation.\n        \"\"\"\n        with torch.no_grad():\n            # Normalize accumulated gradients to prevent explosion\n            grad_w_norm = self.accumulated_grad_w.norm()\n            grad_b_norm = self.accumulated_grad_b.norm()\n            \n            if grad_w_norm > 1e-8:\n                normalized_grad_w = self.accumulated_grad_w / (grad_w_norm + 1e-8)\n                self.slow_weights.data += self.consolidation_rate * normalized_grad_w\n            \n            if grad_b_norm > 1e-8:\n                normalized_grad_b = self.accumulated_grad_b / (grad_b_norm + 1e-8)\n                self.slow_bias.data += self.consolidation_rate * normalized_grad_b\n            \n            # Reset accumulators\n            self.accumulated_grad_w.zero_()\n            self.accumulated_grad_b.zero_()\n    \n    def accumulate_gradients(self):\n        \"\"\"\n        Accumulate gradients for slow weights. Should be called after loss.backward()\n        and before optimizer.step() in training loop.\n        \"\"\"\n        if self.slow_weights.grad is not None:\n            self.accumulated_grad_w += self.slow_weights.grad.detach().clone()\n        if self.slow_bias.grad is not None:\n            self.accumulated_grad_b += self.slow_bias.grad.detach().clone()\n    \n    def reset_state(self):\n        \"\"\"Reset internal state for new sequence.\"\"\"\n        self.compressed_context.zero_()\n        self.step_counter.zero_()\n        self.accumulated_grad_w.zero_()\n        self.accumulated_grad_b.zero_()\n\n\nclass NestedStateSpaceModel(nn.Module):\n    \"\"\"\n    Nested State Space Model with Continuum Memory System.\n    \n    Implements hierarchical memory with three levels:\n    - Fast memory: Updates every step (immediate context)\n    - Medium memory: Updates every C steps (recent patterns)\n    - Slow memory: Updates every CÂ² steps (long-term knowledge)\n    \n    This design enables efficient continual learning by consolidating\n    information at multiple time scales, similar to hippocampal-cortical\n    memory consolidation in the brain.\n    \n    Args:\n        hidden_dim: Dimension of hidden representations\n        state_dim: Dimension of SSM state space\n        fast_freq: Update frequency for fast memory (default: 1)\n        medium_freq: Update frequency for medium memory (default: 10)\n        slow_freq: Update frequency for slow memory (default: 100)\n        dropout: Dropout probability\n    \"\"\"\n    \n    def __init__(\n        self,\n        hidden_dim: int = 256,\n        state_dim: int = 64,\n        fast_freq: int = 1,\n        medium_freq: int = 10,\n        slow_freq: int = 100,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.state_dim = state_dim\n        \n        # Traditional SSM parameters with improved initialization\n        self.A = nn.Parameter(self._init_state_matrix(state_dim))\n        self.B = nn.Parameter(torch.randn(state_dim, hidden_dim) * 0.02)\n        self.C = nn.Parameter(torch.randn(hidden_dim, state_dim) * 0.02)\n        self.D = nn.Parameter(torch.eye(hidden_dim) * 0.1)  # Skip connection\n        \n        # Continuum Memory System (CMS) with three levels\n        self.fast_memory = ContinuumMemoryBlock(\n            input_dim=hidden_dim,\n            hidden_dim=hidden_dim * 2,\n            update_frequency=fast_freq,\n            dropout=dropout,\n        )\n        \n        self.medium_memory = ContinuumMemoryBlock(\n            input_dim=hidden_dim,\n            hidden_dim=hidden_dim * 2,\n            update_frequency=medium_freq,\n            dropout=dropout,\n        )\n        \n        self.slow_memory = ContinuumMemoryBlock(\n            input_dim=hidden_dim,\n            hidden_dim=hidden_dim * 2,\n            update_frequency=slow_freq,\n            dropout=dropout,\n        )\n        \n        # Memory fusion layer with residual connections\n        self.fusion = nn.Sequential(\n            nn.Linear(hidden_dim * 3, hidden_dim * 2),\n            nn.LayerNorm(hidden_dim * 2),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim * 2, hidden_dim),\n        )\n        \n        # Learnable weights for memory level importance\n        self.memory_weights = nn.Parameter(torch.ones(3) / 3)\n        \n        # Final layer norm for stability\n        self.final_norm = nn.LayerNorm(hidden_dim)\n        \n        self.register_buffer('global_step', torch.zeros(1, dtype=torch.long))\n        \n    def _init_state_matrix(self, dim: int) -> torch.Tensor:\n        \"\"\"\n        Initialize state transition matrix A with stable eigenvalues.\n        Uses random orthogonal initialization for better gradient flow.\n        \"\"\"\n        Q = torch.nn.init.orthogonal_(torch.empty(dim, dim))\n        return Q * 0.9  # Scale to ensure stability\n        \n    def forward(\n        self,\n        x: torch.Tensor,\n        return_diagnostics: bool = False,\n    ) -> torch.Tensor | Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass with nested memory updates.\n        \n        Args:\n            x: Input tensor [batch, hidden_dim]\n            return_diagnostics: Whether to return update diagnostics\n            \n        Returns:\n            Output tensor or tuple of (output, diagnostics)\n        \"\"\"\n        batch_size = x.size(0)\n        \n        # Core SSM operation (O(n) complexity maintained)\n        state = torch.zeros(batch_size, self.state_dim, device=x.device, dtype=x.dtype)\n        state = torch.tanh(state @ self.A.t() + x @ self.B.t())\n        ssm_output = state @ self.C.t() + x @ self.D\n        \n        # Multi-level memory processing\n        fast_out, fast_updated = self.fast_memory(ssm_output)\n        medium_out, medium_updated = self.medium_memory(ssm_output)\n        slow_out, slow_updated = self.slow_memory(ssm_output)\n        \n        # Normalize memory weights to sum to 1\n        weights = torch.softmax(self.memory_weights, dim=0)\n        \n        # Weighted combination of memory levels\n        combined = torch.cat([\n            fast_out * weights[0],\n            medium_out * weights[1],\n            slow_out * weights[2],\n        ], dim=-1)\n        \n        # Fuse all memory levels\n        output = self.fusion(combined)\n        \n        # Add residual connection from SSM output\n        output = self.final_norm(output + ssm_output)\n        \n        self.global_step += 1\n        \n        if return_diagnostics:\n            diagnostics = {\n                'global_step': self.global_step.item(),\n                'fast_updated': fast_updated,\n                'medium_updated': medium_updated,\n                'slow_updated': slow_updated,\n                'memory_weights': weights.detach().cpu().tolist(),\n                'fast_freq': self.fast_memory.update_frequency,\n                'medium_freq': self.medium_memory.update_frequency,\n                'slow_freq': self.slow_memory.update_frequency,\n                'fast_alpha': torch.sigmoid(self.fast_memory.alpha).item(),\n                'medium_alpha': torch.sigmoid(self.medium_memory.alpha).item(),\n                'slow_alpha': torch.sigmoid(self.slow_memory.alpha).item(),\n            }\n            return output, diagnostics\n        \n        return output\n    \n    def accumulate_gradients(self):\n        \"\"\"\n        Accumulate gradients for all memory blocks.\n        Call this after loss.backward() in training loop.\n        \"\"\"\n        self.fast_memory.accumulate_gradients()\n        self.medium_memory.accumulate_gradients()\n        self.slow_memory.accumulate_gradients()\n    \n    def reset_state(self):\n        \"\"\"Reset all memory states for new sequence.\"\"\"\n        self.fast_memory.reset_state()\n        self.medium_memory.reset_state()\n        self.slow_memory.reset_state()\n        self.global_step.zero_()\n    \n    def get_memory_utilization(self) -> Dict[str, float]:\n        \"\"\"\n        Get utilization statistics for each memory level.\n        \n        Returns:\n            Dictionary with update counts and frequencies\n        \"\"\"\n        total_steps = self.global_step.item()\n        if total_steps == 0:\n            return {\n                'fast_updates': 0,\n                'medium_updates': 0,\n                'slow_updates': 0,\n                'total_steps': 0,\n            }\n        \n        return {\n            'fast_updates': total_steps // self.fast_memory.update_frequency,\n            'medium_updates': total_steps // self.medium_memory.update_frequency,\n            'slow_updates': total_steps // self.slow_memory.update_frequency,\n            'total_steps': total_steps,\n            'fast_utilization': 1.0,\n            'medium_utilization': min(1.0, self.medium_memory.update_frequency / max(1, total_steps)),\n            'slow_utilization': min(1.0, self.slow_memory.update_frequency / max(1, total_steps)),\n        }\n\n\nclass NestedSSMConfig:\n    \"\"\"\n    Configuration for Nested State Space Model.\n    \n    Attributes:\n        hidden_dim: Dimension of hidden representations\n        state_dim: Dimension of SSM state space\n        fast_freq: Update frequency for fast memory\n        medium_freq: Update frequency for medium memory\n        slow_freq: Update frequency for slow memory\n        dropout: Dropout probability\n    \"\"\"\n    \n    def __init__(\n        self,\n        hidden_dim: int = 256,\n        state_dim: int = 64,\n        fast_freq: int = 1,\n        medium_freq: int = 10,\n        slow_freq: int = 100,\n        dropout: float = 0.1,\n    ):\n        self.hidden_dim = hidden_dim\n        self.state_dim = state_dim\n        self.fast_freq = fast_freq\n        self.medium_freq = medium_freq\n        self.slow_freq = slow_freq\n        self.dropout = dropout\n        \n        # Validate frequencies\n        assert fast_freq <= medium_freq <= slow_freq, \\\n            \"Frequencies must be in ascending order: fast <= medium <= slow\"\n        assert fast_freq > 0, \"Fast frequency must be positive\"\n