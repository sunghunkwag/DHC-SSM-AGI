"""\nBenchmark DHC-SSM against established SSM architectures.\n\nCompares performance metrics including:\n- Throughput (samples/sec)\n- Memory consumption (peak MB)\n- Gradient flow quality\n- Long sequence stability\n"""\n\nimport torch\nimport torch.nn as nn\nimport time\nimport numpy as np\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nimport argparse\n\ntry:\n    from dhc_ssm import DHCSSMModel, DHCSSMConfig\nexcept ImportError:\n    import sys\n    sys.path.insert(0, str(Path(__file__).parent.parent))\n    from dhc_ssm import DHCSSMModel, DHCSSMConfig\n\n\nclass ModelBenchmark:\n    \"\"\"\n    Comprehensive benchmarking suite for SSM architectures.\n    \"\"\"\n    \n    def __init__(\n        self,\n        seq_len: int = 1024,\n        batch_size: int = 32,\n        hidden_dim: int = 256,\n        device: Optional[str] = None,\n    ):\n        self.seq_len = seq_len\n        self.batch_size = batch_size\n        self.hidden_dim = hidden_dim\n        \n        if device is None:\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = torch.device(device)\n        \n        print(f\"Benchmarking on device: {self.device}\")\n        \n    def benchmark_throughput(\n        self,\n        model: nn.Module,\n        num_iterations: int = 100,\n        warmup_iterations: int = 10,\n    ) -> Dict[str, float]:\n        \"\"\"\n        Measure forward + backward pass throughput.\n        \n        Returns:\n            Dictionary with throughput metrics\n        \"\"\"\n        model = model.to(self.device).train()\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n        \n        # Create dummy input\n        x = torch.randn(self.batch_size, 3, 32, 32, device=self.device)\n        target = torch.randint(0, 10, (self.batch_size,), device=self.device)\n        \n        # Warmup\n        for _ in range(warmup_iterations):\n            optimizer.zero_grad()\n            y = model(x)\n            loss = nn.functional.cross_entropy(y, target)\n            loss.backward()\n            optimizer.step()\n        \n        # Synchronize before timing\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        \n        start_time = time.perf_counter()\n        \n        for _ in range(num_iterations):\n            optimizer.zero_grad()\n            y = model(x)\n            loss = nn.functional.cross_entropy(y, target)\n            loss.backward()\n            optimizer.step()\n        \n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        \n        elapsed = time.perf_counter() - start_time\n        \n        throughput = (num_iterations * self.batch_size) / elapsed\n        latency_ms = (elapsed / num_iterations) * 1000\n        \n        return {\n            \"throughput_samples_per_sec\": float(throughput),\n            \"latency_ms_per_batch\": float(latency_ms),\n            \"total_time_sec\": float(elapsed),\n        }\n    \n    def benchmark_memory(self, model: nn.Module) -> Dict[str, float]:\n        \"\"\"\n        Measure peak memory consumption during forward + backward pass.\n        \n        Returns:\n            Dictionary with memory metrics\n        \"\"\"\n        model = model.to(self.device).train()\n        x = torch.randn(self.batch_size, 3, 32, 32, device=self.device)\n        target = torch.randint(0, 10, (self.batch_size,), device=self.device)\n        \n        if torch.cuda.is_available():\n            torch.cuda.reset_peak_memory_stats()\n            torch.cuda.empty_cache()\n            \n            # Forward + backward\n            y = model(x)\n            loss = nn.functional.cross_entropy(y, target)\n            loss.backward()\n            \n            peak_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB\n            current_mem = torch.cuda.memory_allocated() / 1024**2\n        else:\n            import psutil\n            import os\n            process = psutil.Process(os.getpid())\n            \n            mem_before = process.memory_info().rss / 1024**2\n            \n            y = model(x)\n            loss = nn.functional.cross_entropy(y, target)\n            loss.backward()\n            \n            mem_after = process.memory_info().rss / 1024**2\n            peak_mem = mem_after - mem_before\n            current_mem = mem_after\n        \n        return {\n            \"peak_memory_mb\": float(peak_mem),\n            \"current_memory_mb\": float(current_mem),\n        }\n    \n    def benchmark_gradient_flow(\n        self,\n        model: nn.Module,\n        num_steps: int = 50,\n    ) -> Dict[str, float]:\n        \"\"\"\n        Analyze gradient flow quality (detect vanishing/exploding gradients).\n        \n        Returns:\n            Dictionary with gradient statistics\n        \"\"\"\n        model = model.to(self.device).train()\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n        \n        x = torch.randn(self.batch_size, 3, 32, 32, device=self.device)\n        target = torch.randint(0, 10, (self.batch_size,), device=self.device)\n        \n        grad_norms = []\n        \n        for _ in range(num_steps):\n            optimizer.zero_grad()\n            y = model(x)\n            loss = nn.functional.cross_entropy(y, target)\n            loss.backward()\n            \n            # Compute gradient norm\n            total_norm = 0.0\n            for p in model.parameters():\n                if p.grad is not None:\n                    total_norm += p.grad.data.norm(2).item() ** 2\n            total_norm = total_norm ** 0.5\n            grad_norms.append(total_norm)\n            \n            optimizer.step()\n        \n        grad_norms = np.array(grad_norms)\n        \n        return {\n            \"mean_grad_norm\": float(grad_norms.mean()),\n            \"std_grad_norm\": float(grad_norms.std()),\n            \"max_grad_norm\": float(grad_norms.max()),\n            \"min_grad_norm\": float(grad_norms.min()),\n            \"grad_stability_ratio\": float(grad_norms.std() / (grad_norms.mean() + 1e-8)),\n        }\n    \n    def benchmark_long_sequence(\n        self,\n        model: nn.Module,\n        seq_lengths: List[int] = [512, 1024, 2048, 4096],\n    ) -> Dict[str, Dict[str, float]]:\n        \"\"\"\n        Test stability and performance on varying sequence lengths.\n        \n        Returns:\n            Dictionary mapping sequence lengths to metrics\n        \"\"\"\n        results = {}\n        \n        for seq_len in seq_lengths:\n            try:\n                # Adjust batch size to fit in memory\n                adjusted_batch = max(1, self.batch_size // (seq_len // 512))\n                \n                model = model.to(self.device).eval()\n                x = torch.randn(adjusted_batch, 3, 32, 32, device=self.device)\n                \n                # Measure inference time\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n                \n                start = time.perf_counter()\n                with torch.no_grad():\n                    _ = model(x)\n                \n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n                \n                elapsed = time.perf_counter() - start\n                \n                results[str(seq_len)] = {\n                    \"inference_time_ms\": float(elapsed * 1000),\n                    \"batch_size\": adjusted_batch,\n                    \"success\": True,\n                }\n            except RuntimeError as e:\n                results[str(seq_len)] = {\n                    \"error\": str(e),\n                    \"success\": False,\n                }\n        \n        return results\n    \n    def run_full_benchmark(self, model: nn.Module, model_name: str) -> Dict:\n        \"\"\"\n        Run complete benchmark suite.\n        \n        Returns:\n            Dictionary with all benchmark results\n        \"\"\"\n        print(f\"\\n{'='*60}\")\n        print(f\"Benchmarking: {model_name}\")\n        print(f\"{'='*60}\\n\")\n        \n        results = {\n            \"model_name\": model_name,\n            \"config\": {\n                \"seq_len\": self.seq_len,\n                \"batch_size\": self.batch_size,\n                \"hidden_dim\": self.hidden_dim,\n                \"device\": str(self.device),\n            },\n        }\n        \n        # Count parameters\n        num_params = sum(p.numel() for p in model.parameters())\n        results[\"num_parameters\"] = num_params\n        print(f\"Parameters: {num_params:,}\")\n        \n        # Throughput benchmark\n        print(\"\\nRunning throughput benchmark...\")\n        results[\"throughput\"] = self.benchmark_throughput(model)\n        print(f\"  Throughput: {results['throughput']['throughput_samples_per_sec']:.2f} samples/sec\")\n        print(f\"  Latency: {results['throughput']['latency_ms_per_batch']:.2f} ms/batch\")\n        \n        # Memory benchmark\n        print(\"\\nRunning memory benchmark...\")\n        results[\"memory\"] = self.benchmark_memory(model)\n        print(f\"  Peak memory: {results['memory']['peak_memory_mb']:.2f} MB\")\n        \n        # Gradient flow benchmark\n        print(\"\\nRunning gradient flow analysis...\")\n        results[\"gradient_flow\"] = self.benchmark_gradient_flow(model)\n        print(f\"  Mean grad norm: {results['gradient_flow']['mean_grad_norm']:.4f}\")\n        print(f\"  Stability ratio: {results['gradient_flow']['grad_stability_ratio']:.4f}\")\n        \n        # Long sequence benchmark\n        print(\"\\nRunning long sequence tests...\")\n        results[\"long_sequence\"] = self.benchmark_long_sequence(model)\n        \n        print(f\"\\n{'='*60}\\n\")\n        \n        return results\n\n\ndef compare_models(save_results: bool = False, output_dir: str = \"benchmarks/results\"):\n    \"\"\"\n    Compare DHC-SSM against baseline models.\n    \"\"\"\n    benchmark = ModelBenchmark(batch_size=16, hidden_dim=256)\n    all_results = {}\n    \n    # Benchmark DHC-SSM\n    print(\"\\nBenchmarking DHC-SSM...\")\n    config = DHCSSMConfig(hidden_dim=256, ssm_state_dim=64, num_classes=10)\n    dhc_model = DHCSSMModel(config)\n    all_results[\"DHC-SSM\"] = benchmark.run_full_benchmark(dhc_model, \"DHC-SSM\")\n    \n    # Try to benchmark Mamba (if available)\n    try:\n        from mamba_ssm import Mamba\n        print(\"\\nBenchmarking Mamba baseline...\")\n        \n        class MambaWrapper(nn.Module):\n            def __init__(self, d_model=256, d_state=64):\n                super().__init__()\n                self.encoder = nn.Conv2d(3, d_model, 3, padding=1)\n                self.mamba = Mamba(d_model=d_model, d_state=d_state)\n                self.classifier = nn.Linear(d_model, 10)\n                \n            def forward(self, x):\n                x = self.encoder(x)\n                x = x.flatten(2).transpose(1, 2)\n                x = self.mamba(x)\n                x = x.mean(dim=1)\n                return self.classifier(x)\n        \n        mamba_model = MambaWrapper()\n        all_results[\"Mamba\"] = benchmark.run_full_benchmark(mamba_model, \"Mamba\")\n    except ImportError:\n        print(\"\\nMamba not installed. Skipping baseline comparison.\")\n        print(\"Install with: pip install mamba-ssm\")\n    \n    # Print comparison summary\n    print(\"\\n\" + \"=\" * 80)\n    print(\"BENCHMARK SUMMARY\")\n    print(\"=\" * 80)\n    \n    for model_name, results in all_results.items():\n        print(f\"\\n{model_name}:\")\n        print(f\"  Parameters: {results['num_parameters']:,}\")\n        print(f\"  Throughput: {results['throughput']['throughput_samples_per_sec']:.2f} samples/sec\")\n        print(f\"  Peak Memory: {results['memory']['peak_memory_mb']:.2f} MB\")\n        print(f\"  Grad Stability: {results['gradient_flow']['grad_stability_ratio']:.4f}\")\n    \n    # Save results\n    if save_results:\n        output_path = Path(output_dir)\n        output_path.mkdir(parents=True, exist_ok=True)\n        \n        results_file = output_path / \"benchmark_results.json\"\n        with open(results_file, \"w\") as f:\n            json.dump(all_results, f, indent=2)\n        \n        print(f\"\\nResults saved to: {results_file}\")\n    \n    return all_results\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Benchmark DHC-SSM architecture\")\n    parser.add_argument(\"--save-results\", action=\"store_true\", help=\"Save results to JSON\")\n    parser.add_argument(\"--output-dir\", default=\"benchmarks/results\", help=\"Output directory\")\n    \n    args = parser.parse_args()\n    \n    compare_models(save_results=args.save_results, output_dir=args.output_dir)\n