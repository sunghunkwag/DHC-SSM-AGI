# Mathematical Foundations of DHC-SSM\n\n## 1. Complexity Analysis\n\n### Theorem 1: Linear Time Complexity\n\nThe DHC-SSM forward pass achieves **O(n)** time complexity for sequence length n.\n\n#### Proof\n\nFor input sequence \\( x_1, x_2, ..., x_n \\) where \\( x_t \\in \\mathbb{R}^{d_{in}} \\), the state update equation is:\n\n\\[\nh_t = A h_{t-1} + B x_t\n\\]\n\nwhere:\n- \\( A \\in \\mathbb{R}^{d \\times d} \\) is the state transition matrix\n- \\( B \\in \\mathbb{R}^{d \\times d_{in}} \\) is the input projection matrix\n- \\( h_t \\in \\mathbb{R}^{d} \\) is the hidden state at timestep t\n\nThe output is computed as:\n\n\\[\ny_t = C h_t + D x_t\n\\]\n\nwhere:\n- \\( C \\in \\mathbb{R}^{d_{out} \\times d} \\) is the output projection matrix\n- \\( D \\in \\mathbb{R}^{d_{out} \\times d_{in}} \\) is the skip connection matrix\n\n**Per-timestep computational cost:**\n\n1. Matrix multiplication \\( A h_{t-1} \\): **O(d²)** operations\n2. Matrix multiplication \\( B x_t \\): **O(d · d_{in})** operations\n3. Vector addition: **O(d)** operations\n4. Matrix multiplication \\( C h_t \\): **O(d_{out} · d)** operations\n5. Matrix multiplication \\( D x_t \\): **O(d_{out} · d_{in})** operations\n\nTotal per-step: **O(d² + d · d_{in} + d_{out} · d + d_{out} · d_{in})**\n\nFor n timesteps: **O(n · (d² + d · d_{in} + d_{out} · d + d_{out} · d_{in}))**\n\nSince \\( d \\), \\( d_{in} \\), and \\( d_{out} \\) are architecture hyperparameters independent of sequence length n, the overall complexity is:\n\n\\[\n\\boxed{\\mathcal{O}(n)}\n\\]\n\n**Comparison with Transformers:**\n\nTransformer self-attention requires \\( \\mathcal{O}(n^2 · d) \\) complexity due to pairwise token interactions. DHC-SSM achieves **quadratic speedup** for long sequences.\n\n---\n\n## 2. Nested Learning Integration\n\n### 2.1 Continuum Memory System (CMS)\n\nThe CMS implements three-tier memory consolidation inspired by hippocampal-cortical systems:\n\n#### Fast Memory (Synaptic-level)\n\n\\[\nM_{\\text{fast}}(x_t) = W_f \\cdot \\sigma(U_f x_t + b_f) \\quad \\text{(updates every step)}\n\\]\n\n#### Medium Memory (Local consolidation)\n\n\\[\nM_{\\text{medium}}(x_t) = W_m \\cdot \\sigma(U_m \\tilde{x}_t^{(10)} + b_m) \\quad \\text{(updates every 10 steps)}\n\\]\n\nwhere \\( \\tilde{x}_t^{(10)} \\) is the exponential moving average over the last 10 timesteps:\n\n\\[\n\\tilde{x}_t^{(k)} = \\alpha x_t + (1 - \\alpha) \\tilde{x}_{t-1}^{(k)}, \\quad \\alpha = \\frac{2}{k+1}\n\\]\n\n#### Slow Memory (Systems-level consolidation)\n\n\\[\nM_{\\text{slow}}(x_t) = W_s \\cdot \\sigma(U_s \\tilde{x}_t^{(100)} + b_s) \\quad \\text{(updates every 100 steps)}\n\\]\n\n### 2.2 Gradient Accumulation and Consolidation\n\nFor slow weights \\( W_s \\), gradients are accumulated over C steps before parameter updates:\n\n\\[\n\\nabla_{\\text{acc}} W_s^{(t)} = \\nabla_{\\text{acc}} W_s^{(t-1)} + \\nabla_{W_s} \\mathcal{L}(x_t, y_t)\n\\]\n\nParameter updates occur every C steps with normalized consolidation:\n\n\\[\nW_s^{(t+C)} = W_s^{(t)} + \\eta \\cdot \\frac{\\nabla_{\\text{acc}} W_s^{(t)}}{\\|\\nabla_{\\text{acc}} W_s^{(t)}\\|_2 + \\epsilon}\n\\]\n\nwhere:\n- \\( \\eta \\) is the consolidation learning rate (typically 0.01-0.1)\n- \\( \\epsilon = 10^{-8} \\) prevents division by zero\n\n**Neuroscience Motivation:** This mirrors hippocampal replay during sleep, where experiences are consolidated into cortical long-term memory through repeated reactivation and synaptic strengthening.\n\n### 2.3 Memory Fusion\n\nOutputs from all three memory levels are combined with learned weights:\n\n\\[\ny_t = \\text{Fusion}(\\alpha_f M_{\\text{fast}}(x_t) \\oplus \\alpha_m M_{\\text{medium}}(x_t) \\oplus \\alpha_s M_{\\text{slow}}(x_t))\n\\]\n\nwhere \\( \\oplus \\) denotes concatenation, and weights are normalized:\n\n\\[\n(\\alpha_f, \\alpha_m, \\alpha_s) = \\text{softmax}(w_f, w_m, w_s)\n\\]\n\nwith \\( w_f, w_m, w_s \\) being learnable parameters.\n\n---\n\n## 3. Uncertainty Quantification\n\n### 3.1 Aleatoric Uncertainty (Data Uncertainty)\n\nData-dependent noise is modeled via learned heteroscedastic variance:\n\n\\[\n\\sigma^2_{\\text{aleatoric}}(x) = \\exp(f_\\sigma(x))\n\\]\n\nwhere \\( f_\\sigma : \\mathbb{R}^{d_{in}} \\to \\mathbb{R} \\) is a neural network predicting log-variance.\n\nThe negative log-likelihood loss becomes:\n\n\\[\n\\mathcal{L}_{\\text{aleatoric}} = \\frac{1}{2} \\left( \\frac{\\|y - \\hat{y}\\|^2}{\\sigma^2_{\\text{aleatoric}}(x)} + \\log \\sigma^2_{\\text{aleatoric}}(x) \\right)\n\\]\n\n### 3.2 Epistemic Uncertainty (Model Uncertainty)\n\nModel uncertainty is estimated via Monte Carlo Dropout with T stochastic forward passes:\n\n\\[\n\\mu_{\\text{epistemic}} = \\frac{1}{T} \\sum_{t=1}^T f(x; \\theta_t)\n\\]\n\n\\[\n\\sigma^2_{\\text{epistemic}} = \\frac{1}{T} \\sum_{t=1}^T \\left( f(x; \\theta_t) - \\mu_{\\text{epistemic}} \\right)^2\n\\]\n\nwhere \\( \\theta_t \\) represents dropout-perturbed parameters.\n\n### 3.3 Total Predictive Uncertainty\n\nCombining both sources (assuming independence):\n\n\\[\n\\sigma^2_{\\text{total}} = \\sigma^2_{\\text{epistemic}} + \\mathbb{E}[\\sigma^2_{\\text{aleatoric}}]\n\\]\n\nThis decomposition enables:\n1. **Active learning:** Query points with high epistemic uncertainty\n2. **Robust prediction:** Avoid confident predictions on noisy data (high aleatoric)\n3. **Trustworthy AI:** Reject out-of-distribution inputs\n\n---\n\n## 4. State Space Stability Analysis\n\n### 4.1 Eigenvalue Constraints\n\nFor stable recurrent dynamics, the state transition matrix A must have eigenvalues within the unit circle:\n\n\\[\n\\text{spec}(A) \\subset \\{ \\lambda \\in \\mathbb{C} : |\\lambda| < 1 \\}\n\\]\n\n**Initialization Strategy:** We use orthogonal initialization scaled by 0.9:\n\n\\[\nA = 0.9 \\cdot Q\n\\]\n\nwhere \\( Q \\) is a random orthogonal matrix. This ensures:\n- All eigenvalues have magnitude < 1\n- Gradient flow is not obstructed (unlike vanishing gradients in RNNs)\n\n### 4.2 Gradient Flow Through Time\n\nThe gradient with respect to parameters at time \\( t - k \\) is:\n\n\\[\n\\frac{\\partial \\mathcal{L}_t}{\\partial \\theta_{t-k}} = \\frac{\\partial \\mathcal{L}_t}{\\partial h_t} \\prod_{i=1}^{k} \\frac{\\partial h_{t-i+1}}{\\partial h_{t-i}} \\frac{\\partial h_{t-k}}{\\partial \\theta_{t-k}}\n\\]\n\nFor our SSM:\n\n\\[\n\\frac{\\partial h_{t}}{\\partial h_{t-1}} = A \\cdot \\text{diag}(1 - \\tanh^2(z_{t-1}))\n\\]\n\nwhere \\( z_{t-1} = A h_{t-2} + B x_{t-1} \\).\n\nSince \\( \\|A\\|_2 < 1 \\) and \\( 0 < 1 - \\tanh^2(z) \\leq 1 \\), the gradient magnitude decays controllably, avoiding both vanishing and exploding gradients.\n\n---\n\n## 5. Memory Hierarchy Capacity Analysis\n\n### 5.1 Information Bottleneck\n\nEach memory level compresses information at different rates. The effective capacity is bounded by the mutual information:\n\n\\[\nI(X; M_i) \\leq \\log_2(d_i)\n\\]\n\nwhere \\( d_i \\) is the dimensionality of memory level \\( i \\).\n\n**Capacity Allocation:**\n\n| Memory Level | Update Freq | Capacity (bits) | Purpose |\n|--------------|-------------|-----------------|---------|\n| Fast | Every step | \\( \\log_2(512) \\approx 9 \\) bits | Immediate context |\n| Medium | Every 10 steps | \\( \\log_2(512) \\approx 9 \\) bits | Recent patterns |\n| Slow | Every 100 steps | \\( \\log_2(512) \\approx 9 \\) bits | Long-term knowledge |\n\n### 5.2 Forgetting Dynamics\n\nThe decay of information in each level follows an exponential forgetting curve:\n\n\\[\nI_t = I_0 \\cdot e^{-\\lambda_i t}\n\\]\n\nwhere \\( \\lambda_i \\) is the forgetting rate inversely proportional to update frequency:\n\n\\[\n\\lambda_i \\propto \\frac{1}{f_i}\n\\]\n\nThis creates a **hierarchical temporal receptive field** spanning multiple time scales.\n\n---\n\n## 6. Computational Complexity Summary\n\n| Operation | Complexity | Notes |\n|-----------|------------|-------|\n| SSM Forward | \\( \\mathcal{O}(n · d^2) \\) | Linear in sequence length |\n| Fast Memory | \\( \\mathcal{O}(n · d^2) \\) | Every step |\n| Medium Memory | \\( \\mathcal{O}((n/10) · d^2) \\) | Every 10 steps |\n| Slow Memory | \\( \\mathcal{O}((n/100) · d^2) \\) | Every 100 steps |\n| **Total** | **\\( \\mathcal{O}(n · d^2) \\)** | **Linear overall** |\n\n---\n\n## References\n\n1. **Gu, A., & Dao, T. (2024).** \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\" *arXiv:2312.00752*.\n\n2. **Abehrouz, et al. (2025).** \"Nested Learning: The Illusion of Deep Learning Architectures.\" *NeurIPS 2025*. [PDF](https://abehrouz.github.io/files/NL.pdf)\n\n3. **Kendall, A., & Gal, Y. (2017).** \"What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?\" *NeurIPS 2017*.\n\n4. **Hasselmo, M. E. (2017).** \"How We Remember: Brain Mechanisms of Episodic Memory.\" *MIT Press*.\n\n5. **Bengio, Y., Simard, P., & Frasconi, P. (1994).** \"Learning long-term dependencies with gradient descent is difficult.\" *IEEE Transactions on Neural Networks*.\n\n6. **Hochreiter, S., & Schmidhuber, J. (1997).** \"Long Short-Term Memory.\" *Neural Computation*.\n\n---\n\n## Appendix: Notation Summary\n\n| Symbol | Description | Dimension |\n|--------|-------------|-----------|\n| \\( x_t \\) | Input at time t | \\( d_{in} \\) |\n| \\( h_t \\) | Hidden state at time t | \\( d \\) |\n| \\( y_t \\) | Output at time t | \\( d_{out} \\) |\n| \\( A \\) | State transition matrix | \\( d \\times d \\) |\n| \\( B \\) | Input projection matrix | \\( d \\times d_{in} \\) |\n| \\( C \\) | Output projection matrix | \\( d_{out} \\times d \\) |\n| \\( W_f, W_m, W_s \\) | Fast/medium/slow weights | \\( d_{hidden} \\times d \\) |\n| \\( \\sigma^2_{\\text{aleatoric}} \\) | Aleatoric uncertainty | scalar |\n| \\( \\sigma^2_{\\text{epistemic}} \\) | Epistemic uncertainty | scalar |\n